{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Introduction\n",
        "\n",
        "The aim of this lab if to familiarize students with Dimensionality Reduction and Principle Componenets Analysis (PCA).\n",
        "\n",
        "# 1. PCA\n",
        "For this lab we will use the MNIST dataset."
      ],
      "metadata": {
        "id": "pPnACgzp5mR3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc-a-94ZxPR5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import decomposition\n",
        "from sklearn import datasets as sk_datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import typing\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MNIST dataset contains 70,000 images of handwritten digits. The images are grayscale, 28x28 pixels and centered."
      ],
      "metadata": {
        "id": "sf0mZbFl6XR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_trainset = datasets.MNIST(\n",
        "    root='./data', \n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])\n",
        "    )\n",
        "mnist_testset = datasets.MNIST(\n",
        "    root='./data', \n",
        "    train=False, \n",
        "    download=True, \n",
        "    transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])\n",
        "    )"
      ],
      "metadata": {
        "id": "JRmDDqYr6_IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = data.DataLoader(mnist_trainset, batch_size=512, shuffle=True, drop_last=True)\n",
        "test_loader = data.DataLoader(mnist_testset, batch_size=512, drop_last=True)\n",
        "\n",
        "#visualize 10 samples\n",
        "images, _ = train_loader.__iter__().__next__()\n",
        "plt.figure()\n",
        "f, ax = plt.subplots(1,10)\n",
        "for i in range(10):\n",
        "    ax[i].imshow(images[i].reshape(28,28), cmap=\"gray\")"
      ],
      "metadata": {
        "id": "Bf498ejR7siF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As each image is has dimensions 28x28, this results in a 784 vector. However not all pixels are equal. We want to reduce this vector to smaller subset, with the minimum information loss.\n",
        "\n",
        "We will use sklearn [IncrementalPCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html) method for this.\n",
        "\n",
        "Incremental PCA implements the usual Linear PCA analysis, but allows for batch processing.\n",
        "\n",
        "We will reduce the number of components to 3."
      ],
      "metadata": {
        "id": "M3JkLlNV-KBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_model = decomposition.IncrementalPCA(n_components=3)\n",
        "\n",
        "for images, _ in train_loader:\n",
        "    pca_model.partial_fit(images.reshape(512, -1))"
      ],
      "metadata": {
        "id": "MSZozf_P_zPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This now allows us to plot the images in a 3D space."
      ],
      "metadata": {
        "id": "39WRJTntB9BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_3d = list()\n",
        "labels_3d = list()\n",
        "for images, labels in test_loader:\n",
        "    test_3d.append(\n",
        "        pca_model.transform(images.reshape(512, -1))\n",
        "    )\n",
        "    labels_3d.append(labels)\n",
        "test_3d, labels_3d = np.stack(test_3d), np.stack(labels_3d)\n",
        "\n",
        "def get_cmap(n, name='hsv'):\n",
        "    return plt.cm.get_cmap(name, n)\n",
        "\n",
        "cmap = get_cmap(10)\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "for i in range(10):\n",
        "    idx = labels_3d == i\n",
        "    ax.scatter(\n",
        "        test_3d[idx, 0], test_3d[idx, 1], test_3d[idx, 2], # x,y,z\n",
        "        c=cmap(i),\n",
        "        label=i\n",
        "    )\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Q3WCRelCEd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By plotting the test set in a 3D scatter plot, we can see that images of the same digit are roughly clustered together. This is an indication that some information on the digits is not lost, however we need to better understand the fit we should examine the explained variance.\n",
        "\n",
        "Read the documentation of IncrementalPCA and print the explained variance and explained variance ratio. Do you think that 3 components are sufficient to represent each image? Would you say that this is a good representation of the original 784 element vector?\n",
        "\n",
        "Experiment with different number of components to explain 80% of the variance."
      ],
      "metadata": {
        "id": "_uiAabRmFztv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### your code here\n"
      ],
      "metadata": {
        "id": "Dr-FCqfpG5EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall from the lectures, to solve PCA we need the eigenvectors of\n",
        "the covariance matrix so that: $[U,V]=eig(X,X.T)$. However, faster convergence is achieved without using explicit covariance so:  $[U,S,V]=svd(X)$\n",
        "\n",
        "where:\n",
        "* Rows of $U$ are the basis\n",
        "* Diagonal of $S$ are the eigenvalues\n",
        "* Pick the first $k$ rows\n",
        "\n",
        "Pytorch provides a lower level method that allows us to explore $[U, S, V]$ through [pca_lowrank](https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html)\n"
      ],
      "metadata": {
        "id": "jI8wn_u-yW4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images, _ in train_loader:\n",
        "    (U, S, V) = torch.pca_lowrank(images.reshape(512, -1), q=100, center=True, niter=10)\n",
        "U.shape, S.shape, V.shape"
      ],
      "metadata": {
        "id": "U4vUPIo_0jlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$A$ is a data matrix with m samples and n features\n",
        "\n",
        "$V$ columns represent the principal directions\n",
        "\n",
        "$S^2 / (m - 1)$ contains the eigenvalues of $A.T * A / (m - 1)$\n",
        "\n",
        "`matmul(A, V[:, :k])` projects data to the first k principal components."
      ],
      "metadata": {
        "id": "BWrAeRjU37fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, _ = test_loader.__iter__().__next__()\n",
        "images = images.reshape(512, -1)\n",
        "n_components = 3 # experiment with different number of components\n",
        "pca_matrix = torch.matmul(images, V[:, :n_components])\n",
        "pca_matrix.shape"
      ],
      "metadata": {
        "id": "1QIgvwU149eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can also reconstruct the images by reversing the operation"
      ],
      "metadata": {
        "id": "0ErdXPf36lrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructed_images =  torch.matmul(V[:, :n_components], pca_matrix.T).T\n",
        "reconstructed_images.shape"
      ],
      "metadata": {
        "id": "jIU8A4tW6zGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now visualising examples from the two sets we can also visually inspect the quality of the reconstruction. Does the reconstruction match your expectations?"
      ],
      "metadata": {
        "id": "PB7AKEAZ7FzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(2,10)\n",
        "for j in range(2):\n",
        "    for i in range(10):\n",
        "        if j == 0:\n",
        "            ax[j][i].imshow(images[i].reshape(28,28), cmap=\"gray\") # top row shows raw images\n",
        "        else:\n",
        "            ax[j][i].imshow(reconstructed_images[i].reshape(28,28), cmap=\"gray\") # bottom row shows reconstructed"
      ],
      "metadata": {
        "id": "-S6RXXCb7FLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 PCA in practice\n",
        "\n",
        "Now that we have seen how PCA works and have assessed it quantitatively and qualititively, we will train a classifier on the MNIST dataset.\n",
        "\n",
        "First, using pytorch built in methods train a classification MLP model on the raw MNIST image vectors, with number of inputs equal to the 784 (number of pixels), 512 nodes in the hidden layer and 10 nodes in the output layer."
      ],
      "metadata": {
        "id": "Bj0JRhLO81Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### your code here"
      ],
      "metadata": {
        "id": "BooYNonA9Yx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimate the accuracy of the classifier on the test set."
      ],
      "metadata": {
        "id": "Q7cMipBr9jK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### your code here"
      ],
      "metadata": {
        "id": "N_tmAZ9R9pPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now train another classifier on the images reduced using PCA and calculate the classifiers accuracy. How do the two classifiers compare?"
      ],
      "metadata": {
        "id": "L9TLvha59qcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### your code here"
      ],
      "metadata": {
        "id": "fyeaqrsz96Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Feature Selection\n",
        "\n",
        "For this part of the lab we will use the [diabetes](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html) dataset."
      ],
      "metadata": {
        "id": "mfYZhuQLyNxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_db = sk_datasets.load_diabetes()\n",
        "diabetes_db.data.shape"
      ],
      "metadata": {
        "id": "Rb-nljjL-bsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In week 3, we used all features from this dataset to estimate diabetes however in this case we will use the feature selection strategies from the lecture to filter the attributes.\n",
        "\n",
        "First calculate the correlation coefficient of each attribute against Y. Select the attributes with the 10% highest **absolute** coeficient and train a Linear Regression model on them, using pytorch built in methods. "
      ],
      "metadata": {
        "id": "V4dMKNO0_hEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### your code here"
      ],
      "metadata": {
        "id": "Nsy60bDEAUdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the feature selection method for  $Ï‡^2$ statistical independence and repeat the training process.\n",
        "\n",
        "Hint: check scipy package"
      ],
      "metadata": {
        "id": "DT8IqrFkAWZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### your code here"
      ],
      "metadata": {
        "id": "yKM6uClSAo-_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}